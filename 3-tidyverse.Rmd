# Wrangling data with `tidyverse`

A common saying in data science is that about 90% of the effort in an analysis workflow is in getting data wrangled into the right format and shape, and 10% is actual analysis.  In a point and click program like SPSS or XLSTAT we don't think about this as much because the activity of reshaping the data--making it longer or wider as required, finding and cleaning missing values, selecting columns or rows, etc--is often temporally and programmatically separated from the "actual" analysis. 

In `R`, this can feel a bit different because we are using the same interface to manipulate our data and to analyze it.  Sometimes we'll want to jump back out to a spreadsheet program like Excel or even the command line (the "shell" like `bash` or `zsh`) to make some changes.  But in general the tools for manipulating data in `R` are both more powerful and more easily used than doing these activities by hand, and you will make yourself a much more effective analyst by mastering these basic tools.

Here, we are going to emphasize the set of tools from the [`tidyverse`](https://www.tidyverse.org/), which are extensively documented in Hadley Wickham's and Garrett Grolemund's book [*R for Data Science*](https://r4ds.had.co.nz/).  If you want to learn more, start there!

<center>

![The `tidyverse` is associated with this hexagonal iconography.](img/tidyverse-iconography.png)

</center>

Before we move on to actually learning the tools, let's make sure we've got our data loaded up.

```{r reload the beer data in case you started your session over}
library(tidyverse)
beer_data <- read_csv("data/ba_2002.csv")
```


## Subsetting data

R's system for **indexing** data frames is clear and sensible for those who are used to programming languages, but it is not necessarily easy to read.  

A common situation in R is wanting to select some rows and some columns of our data--this is called "**subsetting**" our data.  But this is less easy than it might be for the beginner in R.  Happily, the `tidverse` methods are much easier to read (and modeled after syntax from **SQL**, which may be helpful for some users.)  We are going to focus on 

### `select()` for columns

The first thing we often want to do in a data analysis is to pick a subset of columns, which usually represent variables.  If we take a look at our beer data, we see that, for example, we have some columns that are data about our beers, and some columns that are user-generated responses:

```{r use glimpse() to examine the beer data}
glimpse(beer_data)
```

So, for example, we might want to try to reverse engineer the way that the `rating` is generated from the sub-modalities, in which case perhaps we only want the last 6 columns.  We learned previously that we can do this with numeric indexing:

```{r reminder of base R subsetting}
beer_data[, 9:14]
```

However, this is both difficult for novices to `R` and difficult to read if you are not intimately familiar with the data.  It is also rather fragile--what if someone else rearranged the data in your import file?  You're just selecting the last 6 columns, which are not guaranteed to contain the rating data you're interested in.


The `select()` function in `tidyverse` (actually from the `dplyr` package) is the smarter, easier way to do this.  It works on data frames, and it can be read as "from \<data frame\>, select the columns that meet the criteria we've set." 

`select(<data frame>, <column 1>, <column 2>, ...)`

The simplest way to use `select()` is just to name the columns you want!

```{r using select() with explicit column names}
select(beer_data, appearance, aroma, palate, taste, overall, rating) # note the lack of quoting on the column names
```

This is much clearer to the reader.

You can also use `select()` with a number of helper functions, which use logic to select columns that meet whatever conditions you set.  For example, the `starts_with()` helper function lets us give a set of characters we want columns to start with:

```{r using select() with a character search}
select(beer_data, starts_with("beer"))
```

There are equivalents for the end of column names (`ends_with()`) and text found anywhere in the name (`contains()`). 

You can combine these statements together to get subsets of columns however you want:

```{r combining character search and explicit select()}
select(beer_data, starts_with("beer"), rating, abv)
```

You can also use programmatic logic in `select()` by using the `where()` helper function, which gives you the ability to specify columns by any arbitrary function.

```{r using where() with select() is more advanced}
select(beer_data, where(~is.numeric(.)))
```

Besides being easier to write conditions for than indexing with `[]`, `select()` is code that is much closer to how you or I think about what we're actually doing, making code that is more human readable.

### `filter()` for rows

So `select()` lets us pick which columns we want.  Can we also use it to pick particular observations?  No.  But for that, there's `filter()`.

We learned that, using `[]` indexing, we'd specify a set of rows we want.  If we want the first 10 rows of `beer_data`, we'd write

```{r reminder of how to get rows from a data frame in base R}
beer_data[1:10, ]
```

Again, this is not very human readable, and if we reorganize our rows this won't be useful anymore.  The `tidyverse` answer to this approach is the `filter()` function, which lets you filter your dataset into specific rows according to *data stored in the table itself*.

```{r first example of filter() with conditional logic}
filter(beer_data, abv > 10) # let's get some heavy beers
```

When using `filter()`, we can specify multiple logical conditions.  For example, let's get only Barleywines that are more than 10% ABV.  If we wanted only exact matches, we could use the direct `==` operator:

```{r using the equality operator in filter()}
filter(beer_data, abv > 10, style == "American Barleywine")
```

But this won't return, for example, any beer labeled as an "English Barleywine".  

```{r using the OR operator in filter()}
filter(beer_data, abv > 10, style == "American Barleywine" | style == "English Barleywine")
```

In `R`, the `|` means Boolean `OR`, and the `&` means Boolean `AND`.  We can use these to combine conditions for searching our data table.  But this can be a bit tedious.  The `stringr` package, part of `tidyverse`, gives a lot of utility functions that we can use instead of this effortful searching.

```{r using character searches in filter()}
filter(beer_data, abv > 10, str_detect(style, "Barleywine"))
```

Here, the `str_detect()` function searched for any text that **contains** "Barleywine" in the `style` column.

## Combining steps with the pipe: `%>%`

It isn't hard to imagine a situation in which we want to **both** select some columns and filter some rows.  There are 3 ways we can do this, one of which is going to be the best for most situations.

Let's imagine we want to get only information about the beers, abv, and rating for stouts

First, we can nest functions:

```{r nesting functions}
select(filter(beer_data, str_detect(style, "Stout")), contains("beer"), abv, rating)
```

The problem with this approach is that we have to read it "inside out".  First, `filter()` will happen and get us only beers that match "Stout" in their `style`.  Then `select()` will get columns that match "beer" in their names, `abv`, and `rating`.  Especially as your code gets complicated, this can be very hard to read.

So we might take the second approach: creating intermediates.  We might first `filter()`, store that step somewhere, then `select()`:

```{r storing results as intermediates}
beer_stouts <- filter(beer_data, str_detect(style, "Stout"))
select(beer_stouts, contains("beer"), abv, rating)
```

But now we have this intermediate we don't really need cluttering up our `Environment` tab.  This is fine for a single step, but if you have a lot of steps in your analysis this is going to get old (and confusing) fast.  You'll have to remove a lot of these using the `rm()` command to keep your code clean.

```{r the rm() function deletes objects from the Environment}
rm(beer_stouts)
```

The final method, and what is becoming standard in modern `R` coding, is the **pipe**, which is written in `tidyverse` as `%>%`.  This garbage-looking set of symbols is actually your best friend, you just don't know it yet.  I use this tool constantly in my R programming, but I've been avoiding it up to this point because it's not part of base R (in fact that's no longer strictly true, but it is kind of complicated at the moment).  


OK, enough background, what the heck _is_ a pipe?  The term "pipe" comes from what it does: like a pipe, `%>%` let's whatever is on it's left side flow through to the right hand side.  It is easiest to read `%>%` as "**AND THEN**".  

```{r the mighty pipe!}
beer_data %>%                              # Start with the beer_data
  filter(str_detect(style, "Stout")) %>%   # AND THEN filter to stouts
  select(contains("beer"), abv, rating)    # AND THEN select the beer columns, etc
```

In this example, each place there is a `%>%` I've added a comment saying "AND THEN".  This is because that's exactly what the pipe does: it passes whatever happened in the previous step to the next function.  Specifically, `%>%` passes the **results** of the previous line to the **first argument** of the next line.

### Pipes require that the lefthand side be a single functional command

This means that we can't directly do something like rewrite `sqrt(1 + 2)` with `%>%`:

```{r order of operations with the pipe}
1 + 2 %>% sqrt # this is instead computing 1 + sqrt(2)
```

Instead, if we want to pass binary operationse in a pipe, we need to enclose them in `()` on the line they are in:

```{r parentheses will make the pipe work better}
(1 + 2) %>% sqrt() # Now this computes sqrt(1 + 2) = sqrt(3)
```

More complex piping is possible using the curly braces (`{}`), which create new R environments, but this is more advanced than you will generally need to be.

### Pipes always pass the result of the lefthand side to the *first* argument of the righthand side

This sounds like a weird logic puzzle, but it's not, as we can see if we look at some simple math.  Let's define a function for use in a pipe that computes the difference between two numbers:

```{r an example of a custom function}
subtract <- function(a, b) a - b
subtract(5, 4)
```

If we want to rewrite that as a pipe, we can write:

```{r argument order still matters with piped functions}
5 %>% subtract(4)
```

But we can't write 

```{r the pipe will always send the previous step to the first argument of the next step}
4 %>% subtract(5) # this is actually making subtract(4, 5)
```

We can explicitly force the pipe to work the way we want it to by using `.` **as the placeholder for the result of the lefthand side**:

```{r using the "." pronoun lets you control order in pipes}
4 %>% subtract(5, .) # now this properly computes subtract(5, 4)
```

So, when you're using pipes, make sure that the output of the lefthand side *should* be going into the first argument of the righthand side--this is often but not always the case, especially with non-`tidyverse` functions.

### Pipes are a pain to type

Typing `%>%` is no fun.  But, happily, RStudio builds in a shortcut for you: macOS is `cmd + shift + M`, Windows is `ctrl + shift + M`.

## Make new columns: `mutate()`

You hopefully are starting to be excited by the relative ease of doing some things in R with `tidyverse` that are otherwise a little bit abstruse.  Here's where I think things get really, really cool.  The `mutate()` function *creates a new column in the existing dataset*.  

We can do this easily in base R by setting a new name for a column and using the assign (`<-`) operator, but this is clumsy. Often, we want to create a new column temporarily, or to combine several existing columns.  We can do this using the `mutate()` function.

Let's say that we want to create a quick categorical variable that tells us whether a beer was rated as more than the central value (2.5) in the 5-pt rating scale.  This is kind of like doing a median split, which we'll get to in a moment.

We know that we can use `filter()` to get just the beers with `rating > 2.5`:


```{r using pipes with filter()}
beer_data %>%
  filter(rating > 2.5)
```

But what if we want to be able to just see this?

```{r first example of mutate()}
beer_data %>%
  mutate(better_than_2.5 = rating > 2.5) %>%
  # We'll select just a few columns to help us see the result
  select(beer_name, rating, better_than_2.5) 
```

What does the above function do?

`mutate()` is a very easy way to edit your data mid-pipe.  So we might want to do some calculations, create a temporary variable using `mutate()`, and then continue our pipe.  **Unless we use `<-` to store our `mutate()`d data, the results will be only temporary.**

We can use the same kind of functional logic we've been using in other `tidyverse` commands in `mutate()` to get real, powerful results.  For example, we might want to know if a beer is rated better than the `mean()` of the ratings.  We can do this easily using `mutate()`:

```{r mutate() makes new columns}
# Let's find out the average (mean) rating for these beers
beer_data$rating %>% 
  mean() 

# Now, let's create a column that tells us if a beer is rated > average
beer_data %>%
  mutate(better_than_average = rating > mean(rating)) %>%
  # Again, let's select just a few columns
  select(beer_name, rating, better_than_average)
```

## Split-apply-combine analyses with `group_by()` and `summarize()`

Many basic data analyses can be described as *split-apply-combine*: *split* the data into groups, *apply* some analysis into groups, and then *combine* the results.  

For example, in our `beer_data` we might want to split the data into beer styles, calculate the average overall rating and standard deviation of the rating for each beer style, and the generate a summary table telling us these results.  Using the `filter()` and `select()` commands we've learned so far, you could probably cobble together this analysis without further tools.

However, `tidyverse` provides two powerful tools to do this kind of analysis:

1.  The `group_by()` function takes a data table and groups it by **categorical** values of any column (generally don't try to use `group_by()` on a numeric variable)
2.  The `summarize()` function is like `mutate()` for groups created with `group_by()`: 
  1.  First, you specify 1 or more new columns you want to calculate for each group
  2.  Second, the function produces 1 value for each group for each new column
  
To accomplish the example above, we'd do the following:

```{r split-apply-combine pipeline example}
beer_summary <- 
  beer_data %>%
  group_by(style) %>%                           # we will create a group for each unique style
  summarize(n_beers = n(),                      # n() counts rows in each style
            mean_rating = mean(rating),         # the mean rating for each style
            sd_rating = sd(rating),             # the standard deviation in rating
            se_rating = sd(rating) / sqrt(n())) # multiple functions in 1 row

beer_summary
```

We can use this approach to even get a summary stats table - for example, confidence limits according to the normal distribution:

```{r simple stat summaries with split-apply-combine}
beer_summary %>%
  mutate(lower_limit = mean_rating - 1.96 * se_rating,
         upper_limit = mean_rating + 1.96 * se_rating)
```

Note that in the above example we use `mutate()`, *not* `summarize()`, because we had saved our summarized data.  We could also have calculated `lower_limit` and `upper_limit` directly as part of the `summarize()` statement if we hadn't saved the intermediate.

## Utilities for data management

Honestly, the amount of power in `tidyverse` is way more than we can cover today, and is covered more comprehensively (obviously) by [Wickham and Grolemund](https://r4ds.had.co.nz/).  However, I want to name 4 more utilities we will make a lot of use of today (and you will want to know about for your own work).  

### Rename your columns

Often you will import data with bad column names or you'll realize you need to rename variables during your workflow.  For this, you can use the `rename()` function:

```{r renaming columns}
names(beer_data)

beer_data %>%
  rename(review_text = reviews)
```

You can also rename by position, which is helpful for quick changes:

```{r rename() works with positions as well as explicit names}
beer_data %>%
  rename(review_text = 1)
```

### Relocate your columns

If you `mutate()` columns or just have a big data set with a lot of variables, often you want to move columns around.  This is a pain to do with `[]`, but again `tidyverse` has a utility to move things around easily: `relocate()`.

```{r reordering columns in a tibble}
beer_data %>%
  relocate(beer_name) # giving no other arguments will move to front
```

You can also use `relocate()` to specify positions

```{r using relative positions with relocate()}
beer_data %>%
  relocate(reviews, .after = rating) # move the long text to the end
```

### Sort your data

More frequently, we will want to rearrange our rows, which can be done with `arrange()`.  All you have to do is give `arrange()` one or more columns to sort the data by.  You can use either the `desc()` or the `-` shortcut to sort in reverse order.

```{r arrange() lets you sort your data}
beer_data %>%
  arrange(desc(rating)) %>% # what are the highest rated beers?
  select(beer_name, rating)
```

You can sort alphabetically as well:

```{r arrange() works on both numeric and character data}
beer_data %>%
  arrange(brewery_name, beer_name) %>% # get beers sorted within breweries
  select(brewery_name, beer_name) %>%  # show only relevant columns
  distinct()                           # discard duplicate rows
```

### Pivot tables

Users of Excel may be familiar with the idea of pivot tables.  These are functions that let us make our data tidier.  To quote Wickham and Grolemund:

> here are three interrelated rules which make a dataset tidy:
>
> 1.  Each variable must have its own column.
> 2.  Each observation must have its own row.
> 3.  Each value must have its own cell.

While these authors present "tidiness" of data as an objective property, I'd argue that data is always tidy **for a specific purpose**.  For example, our data is relatively tidy here, except our numerical ratings: we have 6 different ratings for each beer, which means we have encoded an **implicit variable** in the column names: `rating type`.  If we want to use our data for summarization, the form we have is fine.  But if we want to make plots and do some other modeling, this form may be no good to us.

We can use the `pivot_longer()` function to change our data to make the implicit variable explicit and to make our data tidier.

```{r pivoting data tables}
beer_data %>%
  select(beer_name, user_id, appearance:rating) %>% # for clarity
  pivot_longer(cols = appearance:rating,
               names_to = "rating_type",
               values_to = "rating")
```

Now for each unique combination of `beer_name` and `user_id`, we have 6 rows, one for each type of rating they can generate.

Sometimes we want to have "wider" or "untidy" data.  We can use `pivot_wider()` to reverse the effects of `pivot_longer()`.

While the ideas of pivoting can seem simple, they are both powerful and subtly confusing.  We'll be using these tools throughout the rest of the tutorial, so I wanted to give exposure, but mastering them takes trial and error.  I recommend taking a look at the [relevant chapter in Wickham and Grolemund](https://r4ds.had.co.nz/tidy-data.html) for details.

